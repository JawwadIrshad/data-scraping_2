import shutil
import os
import time
import random 
import asyncio
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
import gspread
from google.oauth2.service_account import Credentials
from functools import wraps 
import re
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
from notify_slack import notify_slack

# === PERFORMANCE OPTIMIZATIONS ===
CONCURRENT_PAGES = 10  # Number of browser tabs to use simultaneously
BATCH_SIZE = 100  # Increased from 10 to 100
MAX_RETRIES = 2  # Reduced from 3 to 2
DEFAULT_TIMEOUT = 15000  # 15 seconds instead of 30
NAVIGATION_TIMEOUT = 20000  # 20 seconds for page load

# === Chrome Profile Paths ===
ORIGINAL_PROFILE = Path(r"C:\Users\TK Digital\AppData\Local\Google\Chrome\User Data\Profile 2")
CLONE_PROFILE = Path(r"C:\Users\TK Digital\AppData\Local\Google\Chrome\ScraperProfile")

# === Google Sheets Configuration ===
SHEET_ID = "10pXljhV0o5b6Pt-WtpOlrw4jT8ti0IRBB020T3A71lk"
URLS_SHEET = "urls"  
SCRAPED_DATA_SHEET = "scraped_data"  
CLEAN_DATA_SHEET = "clean"  
MISSING_DATA_SHEET = "missing"  
MULTIPROFILE_SHEET = "multiagent"
DUPLICATE_DATA_SHEET = "duplicate"

CREDENTIALS_FILE = "service_account.json"

XPATHS = {
    "Name": "//*[@id='overview-section']/div/div[1]/div[2]/div[1]/h1",
    "Business_Name": "//*[@id='overview-section']/div/div[1]/div[2]/div[2]/p[1]",
    "Phone": "//div[contains(@class, 'kIPpNw')]//a[contains(@href, 'tel:')]",
    "Address": "//div[contains(@class, 'hTcjfz')]//p[contains(@class, 'idlIli')]",
    "Website": "//div[contains(@class, 'fbBxUf')]//a",
}

EMAIL_REGEX = re.compile(r"[A-Za-z0-9._%+\-]+@[A-Za-z0-9.\-]+\.[A-Za-z]{2,}")

# === Optimized Rate Limiting (Less Conservative) ===
class AsyncRateLimitMonitor:
    def __init__(self, max_requests_per_minute=60):  # Increased from 40
        self.max_requests = max_requests_per_minute
        self.request_times = []
        self.lock = asyncio.Lock()
    
    async def wait_if_needed(self):
        async with self.lock:
            current_time = time.time()
            self.request_times = [t for t in self.request_times if current_time - t < 60]
            
            if len(self.request_times) >= self.max_requests:
                oldest_request = self.request_times[0]
                wait_time = 60 - (current_time - oldest_request) + 1  # Reduced buffer
                if wait_time > 0:
                    print(f"‚è≥ Rate limit reached, waiting {wait_time:.1f}s")
                    await asyncio.sleep(wait_time)
                
                current_time = time.time()
                self.request_times = [t for t in self.request_times if current_time - t < 60]
            
            self.request_times.append(current_time)
    
    def get_remaining_quota(self):
        current_time = time.time()
        self.request_times = [t for t in self.request_times if current_time - t < 60]
        return max(0, self.max_requests - len(self.request_times))

rate_monitor = AsyncRateLimitMonitor()

# === Connection Pool for Page Reuse ===
class PagePool:
    """Manages a pool of browser pages for reuse"""
    def __init__(self, context, size=CONCURRENT_PAGES):
        self.context = context
        self.size = size
        self.pages = []
        self.available = asyncio.Queue()
        self.initialized = False
        
    async def initialize(self):
        """Create the initial pool of pages"""
        if self.initialized:
            return
            
        print(f"üöÄ Initializing {self.size} browser tabs...")
        for i in range(self.size):
            page = await self.context.new_page()
            page.set_default_timeout(DEFAULT_TIMEOUT)
            page.set_default_navigation_timeout(NAVIGATION_TIMEOUT)
            self.pages.append(page)
            await self.available.put(page)
        self.initialized = True
        print(f"‚úÖ Browser pool ready with {self.size} tabs")
    
    async def acquire(self):
        """Get an available page from the pool"""
        return await self.available.get()
    
    async def release(self, page):
        """Return a page to the pool"""
        await self.available.put(page)
    
    async def cleanup(self):
        """Close all pages"""
        for page in self.pages:
            try:
                await page.close()
            except:
                pass

# === Simplified Exponential Backoff ===
def exponential_backoff(retries=2, base_delay=1, max_delay=30):  # Reduced delays
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            for attempt in range(retries):
                try:
                    return await func(*args, **kwargs)
                except gspread.exceptions.APIError as e:
                    error_str = str(e)
                    if "429" not in error_str and "RESOURCE_EXHAUSTED" not in error_str:
                        raise e
                    
                    if attempt == retries - 1:
                        raise e
                    
                    delay = min(base_delay * (2 ** attempt), max_delay)
                    print(f"‚ö†Ô∏è Rate limit hit, waiting {delay:.2f}s")
                    await asyncio.sleep(delay)
                except Exception as e:
                    raise e
                    
            return await func(*args, **kwargs)
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            for attempt in range(retries):
                try:
                    return func(*args, **kwargs)
                except gspread.exceptions.APIError as e:
                    error_str = str(e)
                    if "429" not in error_str and "RESOURCE_EXHAUSTED" not in error_str:
                        raise e
                    
                    if attempt == retries - 1:
                        raise e
                    
                    delay = min(base_delay * (2 ** attempt), max_delay)
                    print(f"‚ö†Ô∏è Rate limit hit, waiting {delay:.2f}s")
                    time.sleep(delay)
                except Exception as e:
                    raise e
                    
            return func(*args, **kwargs)
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    return decorator

# === Optimized Scraping Function ===
async def scrape_single_url_optimized(url: str, page, idx: int, batch_number: int):
    """Scrape a single URL using a provided page (reused)"""
    row_dict = {
        "business_name": "",
        "name": "", 
        "phone": "",
        "address": "",
        "website": "",
        "url": url
    }
    
    scraped = False
    retries = MAX_RETRIES
    
    while not scraped and retries > 0:
        try:
            print(f"   [{batch_number}-{idx}] Scraping: {url[:50]}...")
            
            # Navigate with timeout
            await page.goto(url, wait_until="domcontentloaded", timeout=NAVIGATION_TIMEOUT)
            
            # Quick wait for dynamic content (reduced from typical 3-5 seconds)
            await page.wait_for_timeout(1000)
            
            # Parallel extraction of all fields
            extraction_tasks = [
                safe_text_async(page, XPATHS["Business_Name"]),
                safe_text_async(page, XPATHS["Name"]),
                safe_text_async(page, XPATHS["Phone"]),
                safe_text_async(page, XPATHS["Address"]),
                safe_attr_async(page, XPATHS["Website"], "href"),
            ]
            
            results = await asyncio.gather(*extraction_tasks, return_exceptions=True)
            
            business_name, name, phone, address, website = results
            
            # Handle phone extraction
            if not phone or isinstance(phone, Exception):
                phone_elem = await page.query_selector("//a[contains(@href, 'tel:')]")
                if phone_elem:
                    phone_href = await phone_elem.get_attribute("href")
                    if phone_href and 'tel:' in phone_href:
                        phone = phone_href.replace('tel:', '').strip()
            
            row_dict.update({
                "business_name": business_name if not isinstance(business_name, Exception) else "",
                "name": name if not isinstance(name, Exception) else "",
                "phone": phone if not isinstance(phone, Exception) else "",
                "address": address if not isinstance(address, Exception) else "",
                "website": website if not isinstance(website, Exception) else "",
            })
            
            scraped = True
            print(f"   ‚úÖ [{batch_number}-{idx}] Success: {row_dict['business_name'][:30]} | {row_dict['phone']}")
            
        except PlaywrightTimeoutError:
            print(f"   ‚è±Ô∏è [{batch_number}-{idx}] Timeout - retrying...")
            retries -= 1
            if retries > 0:
                await page.goto("about:blank")  # Clear page
                await asyncio.sleep(0.5)
        except Exception as e:
            print(f"   ‚ö†Ô∏è [{batch_number}-{idx}] Error: {str(e)[:50]}")
            retries -= 1
            if retries > 0:
                await page.goto("about:blank")  # Clear page
                await asyncio.sleep(0.5)
    
    if not scraped:
        print(f"   ‚ùå [{batch_number}-{idx}] Failed after {MAX_RETRIES} attempts")
    
    # Clear page for next use
    try:
        await page.goto("about:blank")
    except:
        pass
    
    return row_dict

async def safe_text_async(page, xpath):
    """Extract text with timeout"""
    try:
        element = await page.wait_for_selector(xpath, timeout=3000, state="attached")
        if element:
            return await element.text_content()
    except:
        return ""

async def safe_attr_async(page, xpath, attribute):
    """Extract attribute with timeout"""
    try:
        element = await page.query_selector(xpath)
        if element:
            return await element.get_attribute(attribute)
    except:
        return ""

# === Batch Processing with Page Pool ===
async def scrape_batch_with_pool(urls_batch, page_pool, all_sheets, batch_number):
    """Process a batch using page pool for maximum concurrency"""
    
    async def process_url(url, idx):
        page = await page_pool.acquire()
        try:
            result = await scrape_single_url_optimized(url, page, idx, batch_number)
            # Mark URL as completed in sheets
            await mark_url_completed_async(url, all_sheets[URLS_SHEET])
            return result
        finally:
            await page_pool.release(page)
    
    # Process all URLs concurrently with page pool
    tasks = [process_url(url, idx) for idx, url in enumerate(urls_batch, start=1)]
    batch_data = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Filter out exceptions
    valid_data = [d for d in batch_data if isinstance(d, dict)]
    
    # Process batch data to sheets
    await process_batch_data_async(valid_data, all_sheets, batch_number)
    
    return len(valid_data)

# === Helper functions (async versions) ===
async def mark_url_completed_async(url, worksheet):
    """Mark URL as completed asynchronously"""
    try:
        # This would need to be implemented with async gspread
        pass
    except:
        pass

async def process_batch_data_async(batch_data, all_sheets, batch_number):
    """Process and save batch data asynchronously"""
    if not batch_data:
        return
    
    # Separate clean and missing data
    clean_data = []
    missing_data = []
    
    for row in batch_data:
        if row.get("business_name") and row.get("phone"):
            clean_data.append(row)
        else:
            missing_data.append(row)
    
    # Save to sheets (would need async implementation)
    print(f"üìä Batch {batch_number}: {len(clean_data)} clean, {len(missing_data)} missing")
    
    # Here you would append to sheets asynchronously

# === Main Optimized Scraper ===
async def scrape_async_optimized():
    """Optimized main scraping function"""
    
    print("üöÄ Starting OPTIMIZED Scraper")
    print(f"‚ö° Performance Settings:")
    print(f"   - Concurrent tabs: {CONCURRENT_PAGES}")
    print(f"   - Batch size: {BATCH_SIZE}")
    print(f"   - Timeout: {DEFAULT_TIMEOUT/1000}s")
    
    # No initial delay needed
    await asyncio.sleep(2)
    
    # Connect to Google Sheets
    print("üîó Connecting to Google Sheets...")
    sheet = connect_gsheet()
    all_sheets = ensure_all_sheets(sheet)
    
    ensure_clone()
    
    total_processed = 0
    start_time = time.time()
    batch_number = 0
    
    async with async_playwright() as p:
        # Launch browser
        context = await p.chromium.launch_persistent_context(
            str(CLONE_PROFILE),
            headless=True,  # Headless is faster
            viewport={"width": 1280, "height": 900},
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-dev-shm-usage",
                "--no-sandbox",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process",
                "--disable-images",  # Don't load images
                "--disable-javascript",  # If possible, disable JS
            ]
        )
        
        # Initialize page pool
        page_pool = PagePool(context, CONCURRENT_PAGES)
        await page_pool.initialize()
        
        # Main processing loop
        while True:
            batch_number += 1
            batch_start_time = time.time()
            
            # Get available URLs
            available_urls = load_input_urls(sheet)
            
            if not available_urls:
                print("‚úÖ All URLs processed!")
                break
            
            # Take batch
            urls_batch = available_urls[:BATCH_SIZE]
            
            print(f"\n{'=' * 70}")
            print(f"üîÑ BATCH {batch_number} - Processing {len(urls_batch)} URLs")
            print(f"üìä Remaining: {len(available_urls) - len(urls_batch)}")
            print(f"{'=' * 70}")
            
            # Process batch with page pool
            scraped_count = await scrape_batch_with_pool(
                urls_batch, page_pool, all_sheets, batch_number
            )
            
            total_processed += scraped_count
            
            # Statistics
            batch_time = time.time() - batch_start_time
            total_time = time.time() - start_time
            current_speed = scraped_count / (batch_time / 60) if batch_time > 0 else 0
            overall_speed = total_processed / (total_time / 60) if total_time > 0 else 0
            
            print(f"\nüìà BATCH {batch_number} COMPLETED")
            print(f"‚úÖ Processed: {scraped_count} URLs in {batch_time:.1f}s")
            print(f"‚ö° Current speed: {current_speed:.1f} URLs/min")
            print(f"üìä Total: {total_processed} URLs")
            print(f"‚è±Ô∏è Overall speed: {overall_speed:.1f} URLs/min")
            
            # Estimate time remaining
            if overall_speed > 0:
                remaining_urls = len(available_urls) - len(urls_batch)
                eta_minutes = remaining_urls / overall_speed
                print(f"‚è≥ ETA: {eta_minutes:.1f} minutes ({eta_minutes/60:.1f} hours)")
            
            # Minimal delay between batches
            if len(available_urls) > BATCH_SIZE:
                await asyncio.sleep(1)  # Just 1 second between batches
        
        # Cleanup
        await page_pool.cleanup()
        await context.close()
    
    # Final statistics
    total_duration = time.time() - start_time
    print(f"\nüéâ SCRAPING COMPLETED!")
    print(f"üìä Total URLs: {total_processed}")
    print(f"‚è±Ô∏è Total time: {total_duration/60:.1f} minutes")
    print(f"‚ö° Average speed: {total_processed/(total_duration/60):.1f} URLs/min")
    
    # Send Slack notification
    message = (
        f"*Optimized Scraping Complete*\n"
        f"URLs Processed: *{total_processed}*\n"
        f"Time: *{total_duration/60:.1f} minutes*\n"
        f"Speed: *{total_processed/(total_duration/60):.1f} URLs/min*"
    )
    notify_slack(message)

# === Google Sheets functions (keep your existing ones) ===
@exponential_backoff(retries=2, base_delay=1)
def connect_gsheet():
    """Connect to Google Sheets using service account credentials"""
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ]
    creds = Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=scopes)
    client = gspread.authorize(creds)
    return client.open_by_key(SHEET_ID)

def ensure_all_sheets(sheet):
    """Ensure all required sheets exist"""
    # Implementation from original script
    pass

def ensure_clone():
    """Ensure Chrome profile clone exists"""
    if CLONE_PROFILE.exists():
        shutil.rmtree(CLONE_PROFILE, ignore_errors=True)
    shutil.copytree(ORIGINAL_PROFILE, CLONE_PROFILE, dirs_exist_ok=True)

def load_input_urls(sheet):
    """Load URLs that haven't been completed yet"""
    # Implementation from original script
    # Should return only URLs with status != "Completed"
    pass

# === Entry Point ===
async def main():
    """Main entry point"""
    await scrape_async_optimized()

if __name__ == "__main__":
    # For even better performance, use uvloop if available
    try:
        import uvloop
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
        print("‚ö° Using uvloop for maximum performance")
    except ImportError:
        pass
    
    asyncio.run(main())